"""
Demo: Visualize attention from each generated token to ALL prompt tokens.

This example shows how each word generated by the model attends back
to every word in the prompt, creating separate plots for each generated word.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from model_loader import AttentionExtractor
from visualizer import AttentionVisualizer
import matplotlib.pyplot as plt

# Initialize
print("Loading TinyStories-33M model...")
extractor = AttentionExtractor(model_name="roneneldan/TinyStories-33M")
viz = AttentionVisualizer()

# Test different prompts
prompts = [
    "The cat is",
    "Once upon a time",
]

print(f"\n{'='*60}")
print("Generating text and analyzing attention to ALL prompt tokens")
print(f"{'='*60}\n")

for i, prompt in enumerate(prompts, 1):
    print(f"{i}. Prompt: {prompt}")

    # Generate with attention tracking to all prompt tokens
    result = extractor.generate_with_attention_to_all_prompt_tokens(
        prompt=prompt,
        max_new_tokens=10,
        temperature=1.0
    )

    print(f"   Generated: {result['generated_text']}")
    print(f"   Prompt tokens: {result['prompt_tokens']}")
    print(f"   Generated tokens: {result['generated_tokens']}\n")

    # Visualize - creates one subplot per generated token
    viz.plot_generated_attention_to_all_prompt_tokens(
        generation_result=result,
        save_path=f"outputs/generation_attention_per_word_{i}.png"
    )
    plt.close()

print("\nâœ… Visualizations saved to outputs/ directory")
print("\nKey insights:")
print("- Each subplot shows one generated token's attention distribution")
print("- Bar height = how much that generated token attends to each prompt token")
print("- Color intensity shows relative attention strength within that subplot")
print("- This reveals which prompt words influence each generated word")
