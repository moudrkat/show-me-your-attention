"""
Demo: Visualize attention from generated tokens to a target word.

This example shows how each word generated by the model attends back
to a specific target word in the prompt.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from model_loader import AttentionExtractor
from visualizer import AttentionVisualizer
import matplotlib.pyplot as plt

# Initialize
print("Loading TinyStories-33M model...")
extractor = AttentionExtractor(model_name="roneneldan/TinyStories-33M")
viz = AttentionVisualizer()

# Test different prompts
prompts = [
    "Scare the cat.",
    "Tell me about the cat.",
    "The cat is sleeping."
]

target_word = "cat"

print(f"\n{'='*60}")
print(f"Target word: '{target_word}'")
print(f"{'='*60}\n")

for i, prompt in enumerate(prompts, 1):
    print(f"{i}. Prompt: {prompt}")

    # Generate with attention tracking
    result = extractor.generate_with_attention_to_target(
        prompt=prompt,
        target_word=target_word,
        max_new_tokens=15,
        temperature=1.0
    )

    print(f"   Generated: {result['generated_text']}")
    print(f"   Avg attention to '{target_word}': {sum(result['attention_scores']) / len(result['attention_scores']):.4f}\n")

    # Visualize
    viz.plot_generated_attention_to_target(
        generation_result=result,
        show_layer_breakdown=False,
        save_path=f"outputs/generation_attention_{i}.png"
    )
    plt.close()

print("\nâœ… Visualizations saved to outputs/ directory")
print("\nKey insights:")
print("- Higher bars = generated token pays more attention to target word")
print("- Color intensity shows relative attention strength")
print("- Early generated tokens often have higher attention to prompt words")
